{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Quantitative analyses of texts are often based initially on counts of the smaller linguistic units that occur within texts, such as the words, the paragraph, or the sentences. The process of dividing a text into such smaller components is referred to as tokenisation. \n",
    "\n",
    "Word tokenisation generally takes place on the basis of the spaces that occur in between the words. The individual words that are found in a text are called “**tokens**\". When this full list is deduplicated, leaving only the unique words, the items in such lists are called “**types**”. \n",
    "\n",
    "This tutorial explains how you can tokenise a text using the `nltk` package, a toolkit that enables you you work with texts in natural languages. \n",
    "\n",
    "To make use of this package, you firstly need to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tdmhNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading tdmh-0.1.7.tar.gz (4.1 kB)\n",
      "Building wheels for collected packages: tdmh\n",
      "  Building wheel for tdmh (setup.py): started\n",
      "  Building wheel for tdmh (setup.py): finished with status 'done'\n",
      "  Created wheel for tdmh: filename=tdmh-0.1.7-py3-none-any.whl size=5610 sha256=10e7b9bf147c411d8a9e2882c634a6cdc0fb461a079d6aba6b6e7eac7dda65df\n",
      "  Stored in directory: c:\\users\\cshed\\appdata\\local\\pip\\cache\\wheels\\55\\c4\\41\\27b73f3965112a0432718f34e168bcd1ffb11897e89f24014d\n",
      "Successfully built tdmh\n",
      "Installing collected packages: tdmh\n",
      "Successfully installed tdmh-0.1.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tdmh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tdmh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods that are discussed in this tutorial make use of a number of additional resources which are not installed by default. If you have never used `nltk` before, you need to run the code below to install these resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cshed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cshed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\cshed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cshed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\cshed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\sentiwordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenisation \n",
    "\n",
    "The `ntlk.tokenize` module has a method named `word_tokenize()`. This methods demand a string as input. This may be a sentence or a whole paragraph. When it is run, the method returns a Python list containing all the individual words found in the string that is provided. As mentioned, the individual words are identified using the spaces found in this string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', ',', 'it', 'was', 'the', 'worst', 'of', 'times']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "quote = '''\n",
    "It was the best of times, \n",
    "it was the worst of times'''\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look closely at the output of the code above, you can see that the `word_tokenize()` method treats punctuation marks as separate tokens. In the example above, the comma following the first occurrence of the word 'times' is actually a separate item in the list. \n",
    "\n",
    "The function `remove_punctuation`, defined below, can be used to remove the tokens that consist of punctuation only. As input, the function demands a list of tokens. For each string in this list, the function tests whether it consists of alphanumeric characters, using the `isalnum()` method. The function returns those words which pass this test only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        if w.isalnum():\n",
    "            new_list.append( w )\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n"
     ]
    }
   ],
   "source": [
    "words = tdmh.remove_punctuation(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created a list containing all the individual tokens, you can easily count the total number of token, using the `len()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text fragment contains 12 tokens.\n"
     ]
    }
   ],
   "source": [
    "nr_tokens = len( words )\n",
    "\n",
    "print( f'The text fragment contains {nr_tokens} tokens.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenisation\n",
    "\n",
    "The method `sent_tokenize()` from the `nltk` package can be used to divide a text into its separate sentences. This type of tokenisation take place on the basis of full stops and upper case characters. \n",
    "\n",
    "The cell below contains an illustration of how the `sent_tokenize()` method can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fragment contains 4 sentences.\n",
      "\n",
      "In the late summer of that year we lived in a house in a village that looked across the river and the plain to the mountains.\n",
      "\n",
      "In the bed of the river there were pebbles and boulders, dry and white in the sun, and the water was clear and swiftly moving and blue in the channels.\n",
      "\n",
      "Troops went by the house and down the road and the dust they raised powdered the leaves of the trees.\n",
      "\n",
      "The trunks of the trees too were dusty and the leaves fell early that year and we saw the troops marching along the road and the dust rising and leaves, stirred by the breeze, falling and the soldiers marching and afterward the road bare and white except for the leaves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## First paragraph of \"A Farewell to Arms\"\n",
    "quote = '''In the late summer of that year we lived in a house in a village that looked across the river and the plain to the mountains. In the bed of the river there were pebbles and boulders, dry and white in the sun, and the water was clear and swiftly moving and blue in the channels. Troops went by the house and down the road and the dust they raised powdered the leaves of the trees. The trunks of the trees too were dusty and the leaves fell early that year and we saw the troops marching along the road and the dust rising and leaves, stirred by the breeze, falling and the soldiers marching and afterward the road bare and white except for the leaves.'''\n",
    "\n",
    "sentences = sent_tokenize(quote)\n",
    "\n",
    "print( f'The fragment contains { len(sentences) } sentences.\\n' )\n",
    "\n",
    "for s in sentences:\n",
    "    print(s + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise 2.1.\n",
    "\n",
    "The 'Corpus' that you have downloaded as part of this tutorial contains the full text of 'Pride and Prejudice'. \n",
    "Write code that can help you to answer the following questions about this text:\n",
    "\n",
    "* How many characters are there in the novel?\n",
    "* How many words does the novel contain?\n",
    "* How often does the novel mention the word \"marriage\"?\n",
    "* How long are the words in *Pride and Prejudice* on average (measured in number of characters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2.\n",
    "\n",
    "Can you print the first five sentences of *Pride and Prejudice*?\n",
    "How many sentences does the novel contain in total?\n",
    "Building on the results of the previous exercise, can you calculate the average length of sentences? In other words, how many tokens do the sentences contain on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize \n",
    "import os\n",
    "\n",
    "path = os.path.join('Corpus', 'PrideandPrejudice.txt')\n",
    "\n",
    "file = open(path, encoding = 'utf-8')\n",
    "full_text = file.read()\n",
    "\n",
    "words = word_tokenize(full_text)\n",
    "sentences = sent_tokenize(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.558501539514197\n"
     ]
    }
   ],
   "source": [
    "nr_words = len(words)\n",
    "nr_sentences = len(sentences)\n",
    "\n",
    "ratio = nr_words/nr_sentences\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3.\n",
    "\n",
    "Can you develop a word tokeniser yourself? Create a function yourself that can take a text as input and which can return a list with all the individual words.\n",
    "\n",
    "In such a function, you can use the `split()` method from `re`. This method enables you to divide a text into smaller parts, based on a regular expression. \n",
    "\n",
    "Also take into account the following requirements: \n",
    "\n",
    "* The function must store all the tokens in a case-insensitive way; the characters must all be stored either in upper or in lower case. \n",
    "* Words can be divided using spaces or the 'em dash', which is written using two condecutive hyphens. \n",
    "* Punctuation marks such as commas, exclampation marks or full stops at the end of a token must be removed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
